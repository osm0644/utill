## 1편
- 강 인공지능 vs. 약 인공지능
 . 강 인공지능: 만능
 . 약 인공지능: 구체적인 특정 업무를 수행하는데 있어서 사람과 비슷한 또는 사람 이상의 성능 
- AI, 머신러닝, 딥러닝
 . AI: 사람이 해야 할 일을 기계가 대신할 수 있는 모든 자동화
 . 머신러닝: 명시적으로 규칙을 프로그래밍하지 않고 데이터로부터 의사결정을 위한 패턴을 기계가 학습 (특징추출: 사람, 판단: 기계)
 . 딥러닝: 인공신경망 기반이 모델로 비정형 데이터로부터 특징 추출 및 판단까지 기계가 한꺼번에 수행 (특징추출: 기계, 판단: 기계)
 
## 2편 기계도 사람처럼 판단할 수 있을까?
- 딥러닝: 머신러닝의 한 분야로 기계가 비정형 데이터를 입력 받은 후 데이터의 주요한 특징을 알아서 추출하고 이를 바탕으로 의사결정을 하는 기술. 머신러닝-정형데이터, 딥러닝-비정형데이터를 주로 다룸 
- 모라벡의 역설: 사람에게 쉬운 것이 기계에게는 어렵다. 기계에게 쉬운 것은 사람에게 어렵다. 
- 인공신경망: 딥러닝 기본 구조로 가중합과 비선형 함수로 이루어진 연산 수행 
 . 원조: 신경세포. 세포의 한쪽끝(수상돌기)에서 받아들인 전기 자극 정보를 이런 저런 처리를 한뒤 다른 쪽 끝(축삭말단)에서 다음 뉴런으로 전달 
 . 인공 뉴런을 다양한 방식으로 여러 층 쌓아 연결 => 인공 신경망 
 . 딥러닝 유행 배경: 1) 다량의 비정형 데이터 확보, 2) 하드웨어 발전 (GPU, TPU 등) 3) 분산저장처리와 같은 빅데이터 처리 기술 

## 3편: 시각을 얻은 인공지능 (Vision AI)
- CNN (Convolutional Neural Network): 이미지 처리에 특화된 신경망
 . 이미지로부터 특징을 추출하는 1) Feature Extraction 영역과 특정 태스크를 수행을 위해 덧붙이는 2) 태스크 수행 영역으로 구성
 . 예시) 교통수단 종류 구별: 컬러, 외관골격, 바퀴모양 등을 파악 1)에서 수행, 1)에서 추출된 정보를 활용하여 탈것의 카테고리를 K개로 구분하는 역할 2)에서 수행
- 특징 추출
 . Convolution 연산(Convolution 필터가 입력이미지를 상하좌우로 훑으며 주요한 특징이 있는지 찾아냄. 찾아낸 특징: Feature Map)과 Pooling 연산(찾아낸 Feature Map을 상하좌우로 훑으며 핵심적인 정보만을 영역별로 샘플링. 주로 영역 내 가장 큰 값만 남기고 나머지 값을 버리는 Max Pooling 방식 적용)으로 이미지로부터 특징 추출 
- 태스크 수행
 1) Classification: 지정된 K개의 클래스(카테고리) 중 하나로 분류 
 2) Detection: 입력 받은 이미지에서 특정 개체가 어디에 위치하는지 (x,y) 좌표값을 찾아주는 과제. 얼굴 인식 등 
 3) Segmentation: 픽셀 단위로 영역을 구별. 자율주행에서 도로, 인도, 차량, 건물 등 구분 
 
## 4편: 언어를 이해하는 인공지능 
- 자연어이해 (NLU: Natural Language Understanding) 
 . NLP는 기계적으로 규칙을 따라 수행. 이해는 맥락 내용에 대한 파악을 전제로 함 (NLU는 NLP 안에 포함됨)
 . NLP: 언어의 형식, 구문론과 관련된 기능. 문장을 형태소 단위로 나누고 구문을 분석하는 등의 기능
 . NLU: 문장 내 표현된 감성을 인식하는 감성분류. 문서 내 중요한 부분을 캐치하는 요약 등의 기능 
- 기계에 언어 인식 절차 
 1) Tokenizing (Parsing): 하나의 문장을 세부단위로 쪼개는 작업 
 2) 워드 임베딩: 토큰을 벡터화 
   - One-hot Encoding: 중복 제거 후 모든 토큰들을 줄세워 사전을 만들고 순서대로 번호를 붙임. 토큰이 다양하고 수가 많을수록 토큰 하나를 표현하기 위해 긴 벡터가 필요 
   - CBOW와 SkipGram: 
     . CBOW: AI에게 문장을 알려주되 중간중간 빈칸을 만들어 들어갈 단어를 유추
     . SKIPGRAM: AI에게 단어 하나를 알려주고 주변에 등장할 만한 문맥을 만들도록 시킴 
- 자연어 이해 과제 
 . 문장/문서 분류(Sentence/Document Classification): 입력받은 텍스트를 지정된 K개의 클래스(카테고리) 중 하나로 분류. 감성 분석과 의도 분류 
 . Sequence-To-Sequence: 문장 또는 문서를 입력으로 받아 문장을 출력. 번역 또는 요약 등 
 . 질의 응답(Question Answering): 사용자 질문이 들어오면 매뉴얼 내에서 가장 답변이 될 가능성이 높은 영역을 리턴하는 MRC(Machine Reading Comprehension)과 가장 유사한 과거 질문/답변을 꺼내주는 IR(Information Retrieval). 상담챗봇 또는 콜센터 
 
## 5편: 시계열 데이터 학습 
- Recurrent Neural Network(RNN: 순환신경망): 과거 데이터를 처리하여 결과를 출력했던 과정의 일부를 가져와 현 시점에서 데이터를 처리하고 결과를 출력하는데 도움을 줌
 . 장점: 
  : 시간 흐름에 따른 과거 정보를 누적할 수 있음
  : 가변 길이의 데이터를 처리할 수 있음(시간단위-timestep 길이에 상관없이 자유롭게 구성 가능) 
  : 다양한 구성의 모델 구성 가능 (one to one, one to many, many to one, many to many 등)  
 . 단점
  : 연산 속도가 느림: 이전 시점의 데이터 처리가 선행되어야 함. 따라서 병렬 학습이 어렵고 순차적으로 처리해야 함. 정형 데이터(수치, 범주형 등)의 경우 속도 저하 체감되지 않고 텍스트 데이터를 다루는 경우 주로 문제가 됨 
  : 학습이 불안정 함: timestep이 길어지면 학습해야 할 값이 폭발적으로 증가=Gradient Exploding, timestep이 길어지면 먼 과거의 이력은 현재 추론에 거의 영향을 미치지 못함=Gradient Vanishing
  : 실질적으로 과거 정보를 잘 활용할 수 있는 모델이 아님=장기 종속성/의존성 문제 
 . 성능 보완 LSTM(Long-short term memory)
  : 먼 과거의 정보 중 중요한 것은 기억하고 불필요한 것은 잊어버리도록 조절 가능한 RNN 유닛 
  : forget gate(불필요하여 버림), input gate(현재 정보를 얼마나 반영할지 결정), output gate(현재 시점에 연산된 최종 정보를 다음 시점에 얼마나 넘길지 결정) 로 구성 

## 6편 오버피팅(Overfitting)과 정규화(Regularization)
- AI Process: Offline/Online Process
 . Offline Process (Training Pipeline)
  : Generate features, Collect labels, Train models, Validate & Select models, Publish model
  : Experimentation(실험)을 계속 => 튜닝 
 . Online Process (학습모델 운영-추론)
  : Load model, Evaluate model
- Generalization: 일반화 성능. 이전에 본적없는 데이터에 대해서도 잘 수행하는 능력 
- 오버피팅(Overfitting): 훈련시에만 잘 작동하고 일반화 성능이 떨어지는 모델 
- 데이터셋: Training, Validation, Test (8:1:1, 6:2:2 정도로 구분)
 . Training set: 모델을 학습하는데 이용하는 데이터 
 . Validation set: 모델을 튜닝(성능개선)하는데 도움을 주는 데이터 
 . Test set: 학습에 전혀 관여하지 않은 데이터로 모델의 최종 성능을 평가하기 위해 따로 떼어놓은 데이터. 모델 성능 비교시 활용
- 학습 곡선 확인: 일반화 성능이 떨어지거나 하여 오버피팅이 발생하지 않았는지 확인
 . 학습이 진행됨에 따라 모델의 성능을 기록하는 그래프. Training set과 Validation set에 대한 모델 성능이 어떻게 변화하는지 확인 
 . Training set에 대해서만 성능 개선이 있고 Validation set에 대해서는 별다른 향상이 없는 시점이 오버피팅 발생 시점 
- Regularization
 . 오버피팅을 피하기 위한 모든 전략으로 일반화 성능을 향상하는데 목적이 있음 
 1) 데이터 증강 (Data Augmentation)
  . 얼마 없는 데이터 건수를 많은 것처럼 데이터를 증강
  . 이미지의 경우 이미지를 좌우 반전시키거나 일부 영역 크롭, 노이즈 추가, 색상/명암/채도 등에 변화를 줌
 2) Capacity 줄이기 
  . 모델의 복잡한 정도를 나타내는 개념
  . 머신러닝 모델보다 딥러닝 모델이 Capacity가 높으며, 신경망을 여러층 쌓거나 뉴런의 수를 많이 둘수록 Capacity가 높아짐
  . Capacity가 높은 모델은 처리할 데이터의 복잡 다양한 패턴을 더 잘 담아낼 수 있지만, 필요이상으로 너무 높은 모델은 데이터를 외우게 됨 (오버피팅)
  . 오버피팅의 경향이 발견된다 싶으면 모델의 층수를 줄여본다던지 한 층의 뉴런 수를 줄인다든지의 조치를 취해볼 필요가 있음
 3) 조기 종료(Early stopping)
  . 오버피팅이 감지될 경우 목표 학습시간이 다 되지 않았다고 하더라도 '조기종료'
 4) 드롭아웃(Dropout)
  . 학습 과정에서 일정 비율 p만큼의 노드(인공뉴런)을 무작위로 끄고 진행하는 기법 
  
## 7편: 인공지능 재활용하기, 전이학습(Transfer Learning)
- 한 번 만들어진 딥러닝 모델을 재활용하여 쓸수 있는 기법. 비슷한 태스크를 다른 도메인에 적용할 때, 그 태스크를 위한 학습 데이터가 부족한 경우 유용하게 활용. 하나의 모델이 이미 배워놓은 지식은 잘 유지하면서도, 새로운 태스크에 대해 필요한 지식을 추가로 습득하게 함 
- 활용 예시
 . A 편의점 절도 감지 모델을 B 편의점에 적용
 . 영화 리뷰 평가에 대한 긍/부정 분류 모델을 뉴스기사 댓글 긍/부정 분류에 적용
 . A 회사의 이메일 보안 위반 탐지 모델을 B 회사에 적용
 . 일상생활의 이미지 분류 모델을 농산품 품종 이미지 구분에 적용 
- Catastrophic forgetting
 . 딥러닝 모델이 새로운 정보를 학습할 때 이전에 배웠던 정보를 완전히 잊어버리는 경향이 발생하는 것 
 => 일반적인 딥러닝 모델은 두 Layer 이상의 인공신경망으로 구성되어 초반에는 데이터의 구체적이고 기본적인 특징을, 후반부로 갈수록 특정 태스크를 위한 추상적이고 개념적인 특징을 학습.  Tranfer Learning을 하려 할 땐 기본특징 학습은 건너뛰고 바로 태스크를 위한 학습으로 하는 것이 좋음
 . 레이어 동결(Layer freezing): 후반부의 신경망 층에 대해서만 파라미터를 학습하고 전반부의 파라미터는 학습되지 않도록 고정하는 기법 
- 모델 이용 예시 
 . Vision: 이미지넷 데이터로 학습된 모델에 Transfer Learning을 적용 
 . NLU: 위키백과 등 방대하고 일반적인 지식에 대해 학습한 적이 있는 모델 이용 
 
## 8편: Pre-trained AI
- 여러 태스크에 활용하기 위해 여러 지식을 미리 두루두루 학습해놓은 인공지능을 만드는 것. (사전학습, 사전학습 모델)
- 보통 특정 데이터타입에 대한 일반적인 지식을 두루 배워놓는 것을 목표로 함. 텍스트 사전학습 모델은 언어의 일반적인 의미와 구조에 대해, 이미지 사전학습 모델은 이미지의 일반적인 특징, 색채, 형태 등에 대해 배움
- 시각 데이터
 . 이미지넷 데이터 (120만 장 데이터에 대해 1000개의 카테고리로 분류)
 . 동영상: Youtube-8M=총 35만 시간에 달하는 610만개의 비디오를 3800개의 카테고리로 다중 분류 
- 언어 데이터 
 . 위키피디아, 나무위키, 세종말뭉치 등 
- Self-Supervised Learning(자가지도학습) 
 . 사람이 만들어주는 정답 라벨이 없어도 기계가 시스템적으로 자체 라벨을 만들어서 사용하는 학습 방법
 . 주로 사전학습에서 이용되며 다량의 데이터는 있으나 라벨은 없는 경우에 활용
 . Self-Supervised Learning의 과제 자체가 의미 있는 것은 아니나, 수많은 데이터를 자체 라벨링으로 학습하게 되면 해당 데이터에 대한 전반적인 지식을 넓고 얕게 습득할 수 있고 이렇게 학습한 모델을 향후 후속 과제로 Transfer Learning하면 맨 땅에서 학습한 모델에 비해 일반적으로 좋은 성능을 보임 
- Self-Supervised Learning을 활용한 Pre-Trained 모델은 다량의 방대한 지식을 골고루 습득하는 것을 목적으로 하기에 대체로 모델의 사이즈가 큰 편이고 사전학습 규모가 어마어마하다는 특징이 있음 

## 9편: Active Learning
- 목적: 라벨링을 위한 예산이 한정되었을때 모델의 성능을 극대화 할 수 있는 라벨링 데이터를 찾기
- 학습 데이터 중 모델 성능 향상에 효과적인 데이터들을 선별한 후, 선별한 데이터를 활용해 학습을 진행하는 방법
- 절차: 목표 성능이 나올때까지 반복 
 . Training a Model : 초기 학습 데이터(labeled data)를 이용해 모델 학습
 . Select Query : 라벨이 되지 않은 데이터 풀로부터 모델에게 도움이 되는 데이터 선별
 . Human Labeling : 선별한 데이터를 사람이 확인하여 라벨을 태깅
 . 선별한 라벨 데이터를 기존 학습 데이터와 병합한 후, 다시 모델을 학습
- 쿼리 전략 
 . 학습된 모델의 판정 값을 기반으로 뽑는 Uncertainty Sampling
 . 여러 개의 모델을 동시에 학습시키면서 많은 모델이 틀리는 데이터를 선별하는 Query by committee
 . 데이터가 학습 데이터로 추가될 때, 학습된 모델이 가장 많이 변화하는 데이터를 선별하는 Expected Impact
 . 데이터가 밀집된 지역의 데이터들을 선별하는 Density weighted method
 . 데이터들을 최대한 고르게 뽑아서 전체 분포를 대표할 수 있도록 데이터를 선별하는 Core-set approach
 
## 10편: Attention mechanism 
- 인공신경망 모델이 특정 영역에 더 집중해서 의사결정하는 방법
- 예) 중요 단어에 조금 더 집중하여 전체 입력을 다시 한번 재조정한 입력 데이터 인코딩 벡터 생성. 중요 단어도 알아서 판단
- 용어 
 . Attention score
  : 인공신경망 모델이 각 인코딩 timestep마다 계산된 특징(feature)를 가지고 자동으로 계산하는 0~1 사이의 값. 각 단어에 대한 주의 집중 가중치 
 . Context vector
  : 어디를 더 살펴보고 어디를 대충 볼지에 대해 어텐션 스코어를 구하고 나면 현재 디코딩할 단어와의 관련성을 반영하여 다시 입력 문장을 인코딩하게 되는데 이는 중요도에 따라 전체 문맥의 정보를 잘 반영하고 있다고 하여 컨텍스트 벡터(Context vector)라고 부름 
- 어텐션 메커니즘이 매번 디코딩마다 직전 단계의 벡터 뿐 아니라 과거의 모든 데이터의 특징(feature)들을 고려하고 딥러닝 모델이 스스로 집중할 영역을 파악 

- XAI (eXplainable AI) 또는 해석가능한 인공지능(interpretable AI)
 . 어텐션 메커니즘은 기계가 판단시 중요하게 생각하는 부분을 우리에게 알려주는 역할도 함. 해당 결정을 내릴 때 어떤 부분에 집중해서 판단했는지를 시각화하여 보여줌 
- Transformer
 . 어텐션만으로 이루어진 인공신경망 구조로 입력 데이터끼리의 self-attention을 통해 상호 정보교환을 수행하는 것이 특징 
 
# 11편: AutoML 
- AutoML 역할 
 . Feature Engineering 자동화: AI 모델을 학습하기 위해 데이터로부터 중요한 특징(feature)을 선택하고 인코딩하는 방식에 대한...
 . 하이퍼파라미터 자동으로 탐색: AI 모델 학습에 필요한 설정 등 탐색 
 . 아키텍처 탐색: AI 모델의 구조 자체를 더 효율적인 방향으로 찾아줌 

 
